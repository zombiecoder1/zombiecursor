version: '3.8'

services:
  zombiecursor:
    build: .
    container_name: zombiecursor
    ports:
      - "5051:5051"
    volumes:
      - ./data:/app/data
      - ./vectorstores:/app/vectorstores
      - ./logs:/app/logs
    environment:
      - HOST=0.0.0.0
      - PORT=5051
      - DEBUG=false
      - LOG_LEVEL=INFO
      - LLM_HOST=http://ollama:11434
      - LLM_MODEL=llama2
      - PROJECT_ROOT=/app
      - MAX_CONTEXT_FILES=50
      - MAX_CONTEXT_SIZE=3000
      - ENABLE_MEMORY=true
      - ENABLE_STREAMING=true
      - VECTOR_STORE_PATH=/app/vectorstores/data
      - EMBEDDING_MODEL=all-MiniLM-L6-v2
      - ENABLE_GIT_TOOL=true
      - ENABLE_PYTHON_TOOL=true
      - ENABLE_SYSTEM_TOOL=true
      - PYTHON_TIMEOUT=30
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - zombiecursor-network

  ollama:
    image: ollama/ollama:latest
    container_name: zombiecursor-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    networks:
      - zombiecursor-network
    entrypoint: ["/bin/bash", "-c", "ollama serve & sleep 5 && ollama pull llama2 && ollama pull codellama && wait"]

  redis:
    image: redis:7-alpine
    container_name: zombiecursor-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    networks:
      - zombiecursor-network
    command: redis-server --appendonly yes

  nginx:
    image: nginx:alpine
    container_name: zombiecursor-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - zombiecursor
    restart: unless-stopped
    networks:
      - zombiecursor-network

volumes:
  ollama_data:
    driver: local
  redis_data:
    driver: local

networks:
  zombiecursor-network:
    driver: bridge